{"cells":[{"cell_type":"code","source":["spark.conf.set(\"dfs.adls.oauth2.access.token.provider.type\", \"ClientCredential\")\nspark.conf.set(\"dfs.adls.oauth2.client.id\", \"3fcabce8-ce4a-4b9f-a66f-f6ef9f7afe0f\") #<APPLICATION-ID>\nspark.conf.set(\"dfs.adls.oauth2.credential\", \"DjdrDC5kmdcQl0L9qUsb0FUI9MJtVUYKCxeJLFHnHxk=\") #<AUTHENTICATION-KEY>\nspark.conf.set(\"dfs.adls.oauth2.refresh.url\", \"https://login.microsoftonline.com/d646b43b-7348-4200-b89c-086fc797aab3/oauth2/token\") #<TENANT-ID>"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["from pyspark.mllib.tree import RandomForest, RandomForestModel\nfrom pyspark.mllib.util import MLUtils\nfrom pyspark import SparkContext\n\n# sc = SparkContext()\ndata = MLUtils.loadLibSVMFile(sc, 'wasbs://spark@testmjhaha9354.blob.core.windows.net/mnt/PUBG_train (3).csv') .collect() - errorë‚¨...\n\ndf = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"adl://sparktestmj.azuredatalakestore.net/spark/PUBG_train.csv\"); \n\ndf_rdd = sc.textFile(\"wasbs://spark@testmjhaha9354.blob.core.windows.net/mnt/PUBG_train (3).csv\")\n\n(training, testing) = df_rdd.randomSplit([0.7, 0.3])\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\nfrom pyspark.mllib.evaluation import RegressionMetrics\ndata = MLUtils.loadLibSVMFile(sc, 'wasbs://spark@testmjhaha9354.blob.core.windows.net/mnt/PUBG_train (3).csv', 176894) \n(training, testing) = data.randomSplit([0.7, 0.3])\n\nmodel = LinearRegressionWithSGD.train(training)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-4474461129709928&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansiyellow\">(</span>training<span class=\"ansiyellow\">,</span> testing<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">=</span> data<span class=\"ansiyellow\">.</span>randomSplit<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0.7</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">0.3</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 6</span><span class=\"ansiyellow\"> </span>model <span class=\"ansiyellow\">=</span> LinearRegressionWithSGD<span class=\"ansiyellow\">.</span>train<span class=\"ansiyellow\">(</span>training<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/regression.pyc</span> in <span class=\"ansicyan\">train</span><span class=\"ansiblue\">(cls, data, iterations, step, miniBatchFraction, initialWeights, regParam, regType, intercept, validateData, convergenceTol)</span>\n<span class=\"ansigreen\">    288</span>                                  float(convergenceTol))\n<span class=\"ansigreen\">    289</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 290</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> _regression_train_wrapper<span class=\"ansiyellow\">(</span>train<span class=\"ansiyellow\">,</span> LinearRegressionModel<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">,</span> initialWeights<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    291</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    292</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/regression.pyc</span> in <span class=\"ansicyan\">_regression_train_wrapper</span><span class=\"ansiblue\">(train_func, modelClass, data, initial_weights)</span>\n<span class=\"ansigreen\">    206</span> <span class=\"ansigreen\">def</span> _regression_train_wrapper<span class=\"ansiyellow\">(</span>train_func<span class=\"ansiyellow\">,</span> modelClass<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">,</span> initial_weights<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    207</span>     <span class=\"ansigreen\">from</span> pyspark<span class=\"ansiyellow\">.</span>mllib<span class=\"ansiyellow\">.</span>classification <span class=\"ansigreen\">import</span> LogisticRegressionModel<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 208</span><span class=\"ansiyellow\">     </span>first <span class=\"ansiyellow\">=</span> data<span class=\"ansiyellow\">.</span>first<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    209</span>     <span class=\"ansigreen\">if</span> <span class=\"ansigreen\">not</span> isinstance<span class=\"ansiyellow\">(</span>first<span class=\"ansiyellow\">,</span> LabeledPoint<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    210</span>         <span class=\"ansigreen\">raise</span> TypeError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;data should be an RDD of LabeledPoint, but got %s&quot;</span> <span class=\"ansiyellow\">%</span> type<span class=\"ansiyellow\">(</span>first<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">first</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">   1397</span>         ValueError<span class=\"ansiyellow\">:</span> RDD <span class=\"ansigreen\">is</span> empty<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1398</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">-&gt; 1399</span><span class=\"ansiyellow\">         </span>rs <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1400</span>         <span class=\"ansigreen\">if</span> rs<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1401</span>             <span class=\"ansigreen\">return</span> rs<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">take</span><span class=\"ansiblue\">(self, num)</span>\n<span class=\"ansigreen\">   1379</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1380</span>             p <span class=\"ansiyellow\">=</span> range<span class=\"ansiyellow\">(</span>partsScanned<span class=\"ansiyellow\">,</span> min<span class=\"ansiyellow\">(</span>partsScanned <span class=\"ansiyellow\">+</span> numPartsToTry<span class=\"ansiyellow\">,</span> totalParts<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1381</span><span class=\"ansiyellow\">             </span>res <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">.</span>runJob<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> takeUpToNumLeft<span class=\"ansiyellow\">,</span> p<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1382</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1383</span>             items <span class=\"ansiyellow\">+=</span> res<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/context.pyc</span> in <span class=\"ansicyan\">runJob</span><span class=\"ansiblue\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansigreen\">   1040</span>         <span class=\"ansired\"># SparkContext#runJob.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1041</span>         mappedRDD <span class=\"ansiyellow\">=</span> rdd<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span>partitionFunc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1042</span><span class=\"ansiyellow\">         </span>sock_info <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>runJob<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jsc<span class=\"ansiyellow\">.</span>sc<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> mappedRDD<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">,</span> partitions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1043</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>sock_info<span class=\"ansiyellow\">,</span> mappedRDD<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1044</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 79.0 failed 4 times, most recent failure: Lost task 0.3 in stage 79.0 (TID 126, 10.139.64.4, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 262, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 257, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 373, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1375, in takeUpToNumLeft\n    yield next(iterator)\n  File &quot;/databricks/spark/python/pyspark/rddsampler.py&quot;, line 95, in func\n    for obj in iterator:\n  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 55, in wrapper\n    return f(*args, **kwargs)\n  File &quot;/databricks/spark/python/pyspark/mllib/util.py&quot;, line 123, in &lt;lambda&gt;\n    parsed = lines.map(lambda l: MLUtils._parse_libsvm_line(l))\n  File &quot;/databricks/spark/python/pyspark/mllib/util.py&quot;, line 48, in _parse_libsvm_line\n    label = float(items[0])\nValueError: could not convert string to float: Id,groupId,matchId,assists,boosts,damageDealt,DBNOs,headshotKills,heals,killPlace,killPoints,kills,killStreaks,longestKill,maxPlace,numGroups,revives,rideDistance,roadKills,swimDistance,teamKills,vehi\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:317)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:457)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:440)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:271)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:182)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:182)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2181)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2181)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:384)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1747)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1735)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1734)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1734)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:962)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:962)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:962)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1970)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1918)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1906)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2141)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2162)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2181)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:182)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 262, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 257, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 373, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1375, in takeUpToNumLeft\n    yield next(iterator)\n  File &quot;/databricks/spark/python/pyspark/rddsampler.py&quot;, line 95, in func\n    for obj in iterator:\n  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 55, in wrapper\n    return f(*args, **kwargs)\n  File &quot;/databricks/spark/python/pyspark/mllib/util.py&quot;, line 123, in &lt;lambda&gt;\n    parsed = lines.map(lambda l: MLUtils._parse_libsvm_line(l))\n  File &quot;/databricks/spark/python/pyspark/mllib/util.py&quot;, line 48, in _parse_libsvm_line\n    label = float(items[0])\nValueError: could not convert string to float: Id,groupId,matchId,assists,boosts,damageDealt,DBNOs,headshotKills,heals,killPlace,killPoints,kills,killStreaks,longestKill,maxPlace,numGroups,revives,rideDistance,roadKills,swimDistance,teamKills,vehi\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:317)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:457)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:440)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:271)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:182)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:182)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2181)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2181)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:384)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["training.count\n\n# model = RandomForest.trainClassifier(training, numClasses=2,numTrees=3, categoricalFeaturesInfo={}, featureSubsetStrategy=\"auto\",impurity='gini', maxDepth=4, maxBins=32)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">47</span><span class=\"ansired\">]: </span>&lt;bound method PipelinedRDD.count of PythonRDD[375] at RDD at PythonRDD.scala:51&gt;\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AssertionError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-4474461129709929&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>model <span class=\"ansiyellow\">=</span> RandomForest<span class=\"ansiyellow\">.</span>trainClassifier<span class=\"ansiyellow\">(</span>training<span class=\"ansiyellow\">,</span> numClasses<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span>numTrees<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">,</span> categoricalFeaturesInfo<span class=\"ansiyellow\">=</span><span class=\"ansiyellow\">{</span><span class=\"ansiyellow\">}</span><span class=\"ansiyellow\">,</span> featureSubsetStrategy<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;auto&quot;</span><span class=\"ansiyellow\">,</span>impurity<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&apos;gini&apos;</span><span class=\"ansiyellow\">,</span> maxDepth<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">4</span><span class=\"ansiyellow\">,</span> maxBins<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">32</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/tree.py</span> in <span class=\"ansicyan\">trainClassifier</span><span class=\"ansiblue\">(cls, data, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seed)</span>\n<span class=\"ansigreen\">    404</span>         return cls._train(data, &quot;classification&quot;, numClasses,\n<span class=\"ansigreen\">    405</span>                           categoricalFeaturesInfo<span class=\"ansiyellow\">,</span> numTrees<span class=\"ansiyellow\">,</span> featureSubsetStrategy<span class=\"ansiyellow\">,</span> impurity<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 406</span><span class=\"ansiyellow\">                           maxDepth, maxBins, seed)\n</span><span class=\"ansigreen\">    407</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    408</span>     <span class=\"ansiyellow\">@</span>classmethod<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/tree.py</span> in <span class=\"ansicyan\">_train</span><span class=\"ansiblue\">(cls, data, algo, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seed)</span>\n<span class=\"ansigreen\">    304</span>                featureSubsetStrategy, impurity, maxDepth, maxBins, seed):\n<span class=\"ansigreen\">    305</span>         first <span class=\"ansiyellow\">=</span> data<span class=\"ansiyellow\">.</span>first<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 306</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">assert</span> isinstance<span class=\"ansiyellow\">(</span>first<span class=\"ansiyellow\">,</span> LabeledPoint<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;the data should be RDD of LabeledPoint&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    307</span>         <span class=\"ansigreen\">if</span> featureSubsetStrategy <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">in</span> cls<span class=\"ansiyellow\">.</span>supportedFeatureSubsetStrategies<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    308</span>             <span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;unsupported featureSubsetStrategy: %s&quot;</span> <span class=\"ansiyellow\">%</span> featureSubsetStrategy<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AssertionError</span>: the data should be RDD of LabeledPoint</div>"]}}],"execution_count":5},{"cell_type":"code","source":["target.shape()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-4474461129709930&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>target<span class=\"ansiyellow\">.</span>shape<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: &apos;Column&apos; object is not callable</div>"]}}],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"python_analysis","notebookId":4474461129709925},"nbformat":4,"nbformat_minor":0}
